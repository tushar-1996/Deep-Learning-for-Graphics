{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL4GV_Ex_7_PointNet (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGHdmTwfoFyK"
      },
      "source": [
        "# PointNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcjpsqV-Yv43"
      },
      "source": [
        "In this exercise you will implement a simple version of the PointNet architecture for point cloud processing and train it to classify point cloud from the **ModelNet10** dataset (https://modelnet.cs.princeton.edu/).\n",
        "\n",
        "First load the necessary dependencies by executing the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uPArEU8gdwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162c6f25-5a00-4a20-86c8-2342d3981d6c"
      },
      "source": [
        "#Numpy\n",
        "import numpy as np\n",
        "#Library used to load the data\n",
        "import h5py\n",
        "\n",
        "#!pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "try:\n",
        "  %tensorflow_version 2.x  # Colab only.\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # Colab only.`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzr2LqYw7v5T"
      },
      "source": [
        "The first thing we will need to do is downloading the data. \n",
        "\n",
        "**ModelNet** is a dataset of different CAD objects. Two of the most common benchmarks used to evaluate the performance of a network to classify point clouds is **ModelNet10** and ModelNet40. **ModelNet40** is composed of almost 10k objects from 40 different classes and ModelNet10 is composed of almost 4k objects from 10 different classes. \n",
        "\n",
        "We have prepared a sampled version of ModelNet10 in which each object is sampled with 512 points. In order to download these files execute the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvbzHT7RjiUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30585534-11d3-4fca-def2-4d20c6459153"
      },
      "source": [
        "!wget https://www.dropbox.com/s/449t6c267kzspfs/modelnet10.zip\n",
        "!unzip modelnet10.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-20 09:32:39--  https://www.dropbox.com/s/449t6c267kzspfs/modelnet10.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.67.18, 2620:100:6020:18::a27d:4012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.67.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/449t6c267kzspfs/modelnet10.zip [following]\n",
            "--2021-01-20 09:32:39--  https://www.dropbox.com/s/raw/449t6c267kzspfs/modelnet10.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucfc8fdf0a87139e4e31d1326f69.dl.dropboxusercontent.com/cd/0/inline/BHWZNmQrU7Y-8vA2gd4r2D9B1IieHs3eg3M2gucEjxW1JRMR7jWEcKDeABkYI_zitHCu2EQtGG1ngZnwlFY6UJQwa95OrFCBlDEsf-fh209vXjs3Njx0ehpP1RFjplluKBg/file# [following]\n",
            "--2021-01-20 09:32:40--  https://ucfc8fdf0a87139e4e31d1326f69.dl.dropboxusercontent.com/cd/0/inline/BHWZNmQrU7Y-8vA2gd4r2D9B1IieHs3eg3M2gucEjxW1JRMR7jWEcKDeABkYI_zitHCu2EQtGG1ngZnwlFY6UJQwa95OrFCBlDEsf-fh209vXjs3Njx0ehpP1RFjplluKBg/file\n",
            "Resolving ucfc8fdf0a87139e4e31d1326f69.dl.dropboxusercontent.com (ucfc8fdf0a87139e4e31d1326f69.dl.dropboxusercontent.com)... 162.125.67.15, 2620:100:6020:15::a27d:400f\n",
            "Connecting to ucfc8fdf0a87139e4e31d1326f69.dl.dropboxusercontent.com (ucfc8fdf0a87139e4e31d1326f69.dl.dropboxusercontent.com)|162.125.67.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BHViwx9J1QYgLRefEh0OFmlXRqco5JDLLqaZDl83BNg7Q6sCTxD7dgs2804aIA6gm1_YdrFT1UGQmPx0rshGIi_-t12onuSSws576L2UaZMKwIpRgDU4ATT_yjJA5VAUz7QQ6nC96yQ7OO1cdF8CD700hSe83gMdVbEeyih5VGVI39f0RZSICgc7AgiWp7nQIEkj6Jmuu4B9ejebaykkX8OYoTgWD9ijRiOmFbQ8HVPVpzDdaC3kMgTcuFt_kYenq3TOt5dRhBXpLl0NyqK3MKoBckGxvfRz55RrvSoAZBPhT1_rIrIonRHqMeFVOo0RS7QPReZmAj_xhUtGq4ZkYGD6HoNLRQPw-B_eN2QYPFmkrA/file [following]\n",
            "--2021-01-20 09:32:40--  https://ucfc8fdf0a87139e4e31d1326f69.dl.dropboxusercontent.com/cd/0/inline2/BHViwx9J1QYgLRefEh0OFmlXRqco5JDLLqaZDl83BNg7Q6sCTxD7dgs2804aIA6gm1_YdrFT1UGQmPx0rshGIi_-t12onuSSws576L2UaZMKwIpRgDU4ATT_yjJA5VAUz7QQ6nC96yQ7OO1cdF8CD700hSe83gMdVbEeyih5VGVI39f0RZSICgc7AgiWp7nQIEkj6Jmuu4B9ejebaykkX8OYoTgWD9ijRiOmFbQ8HVPVpzDdaC3kMgTcuFt_kYenq3TOt5dRhBXpLl0NyqK3MKoBckGxvfRz55RrvSoAZBPhT1_rIrIonRHqMeFVOo0RS7QPReZmAj_xhUtGq4ZkYGD6HoNLRQPw-B_eN2QYPFmkrA/file\n",
            "Reusing existing connection to ucfc8fdf0a87139e4e31d1326f69.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20265172 (19M) [application/zip]\n",
            "Saving to: ‘modelnet10.zip’\n",
            "\n",
            "modelnet10.zip      100%[===================>]  19.33M  11.4MB/s    in 1.7s    \n",
            "\n",
            "2021-01-20 09:32:43 (11.4 MB/s) - ‘modelnet10.zip’ saved [20265172/20265172]\n",
            "\n",
            "Archive:  modelnet10.zip\n",
            "  inflating: modelnet10.hdf5         \n",
            "  inflating: modelnet10_cats.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8McDpknj-D1P"
      },
      "source": [
        "If everything went well you should see the hdf5 file in the associated files of the notebook. Now we will prepare the data for training.\n",
        "\n",
        "First we will load the hdf5 binary file and extract the different datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZleQfSt_Jf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d640ffe7-2975-4b9b-966d-3d1abc1bb351"
      },
      "source": [
        "dataset = h5py.File(\"modelnet10.hdf5\", \"r\")\n",
        "init_x_train = dataset['train_data'][:]\n",
        "init_y_train = dataset['train_categories'][:]\n",
        "x_test = dataset['test_data'][:, :, :]\n",
        "y_test = dataset['test_categories'][:]\n",
        "\n",
        "print(\"Point cloud training:\", init_x_train.shape[0])\n",
        "print(\"Point cloud testing:\", x_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Point cloud training: 3991\n",
            "Point cloud testing: 908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMpsfwVw_Z7N"
      },
      "source": [
        "In order to increase our training data and improve generalization, we are going to augment our training set by scaling it and by applying noise to each point coordinate. This will prevent the network to memorize each model in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V535zyk3YB0"
      },
      "source": [
        "#Number of data augmentation passes.\n",
        "numAugment = 5\n",
        "\n",
        "#Initialize the random seed.\n",
        "np.random.seed(0)\n",
        "\n",
        "#Create the tensor for the augmented data.\n",
        "x_train = np.full((init_x_train.shape[0]*numAugment, 512, 3), \\\n",
        "                  0.0, dtype=np.float32)\n",
        "y_train = np.full((init_x_train.shape[0]*numAugment), 0, dtype=np.int32)\n",
        "\n",
        "#For each model.\n",
        "for curModel in range(init_x_train.shape[0]):\n",
        "  #For each augmentation pass\n",
        "  for i in range(numAugment):\n",
        "    \n",
        "    #Compute a random scaling in each axis between 0.9 and 1.1\n",
        "    scaling = np.random.random((3))*0.2+0.9\n",
        "    x_train[curModel*numAugment + i, :, :] = \\\n",
        "      init_x_train[curModel, :, :].reshape((512,3))*scaling\n",
        "    \n",
        "    #Apply gaussian noise to each point coordinate with a stdev of 0.02.\n",
        "    jittered_data = np.clip(0.02 * np.random.randn(512, 3), -0.1, 0.1)\n",
        "    x_train[curModel*numAugment + i, :, :] = \\\n",
        "      x_train[curModel*numAugment + i, :, :] + jittered_data\n",
        "    \n",
        "    #Save the label for the augmented data.\n",
        "    y_train[curModel*numAugment + i] = \\\n",
        "      init_y_train[curModel]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OALDYry0Inmq"
      },
      "source": [
        "At this point we should have our dataset ready and we can start creating our PointNet network. The original paper contains multiple layers and transformation networks. However, in this exercise we will implement a much simpler version.\n",
        "\n",
        "1.   First, we will create a MLP with two hidden layers (64 and 128 outputs) that transform our points to 128 dimensional points. (Each layer: Dense+BatchNorm+RELU)\n",
        "2.   Then, we will aggregate all the points using a max pooling operation. For this we will use the low level API function tf.reduce_max. This function reduces a tensor along a dimension using the max operation.\n",
        "3.   Lastly, we will apply another MLP to this global feature vector with two hidden layers (32 and 10 outputs) followed by a softmax activation function. We will apply dropout in this MLP with a rate of 0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9YETLadz4vQ"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(512, 3), name='batch_point_cloud')\n",
        "\n",
        "################# TODO \n",
        "#MLP to transform points to 64 dimensions\n",
        "x = tf.keras.layers.Dense(64)(inputs)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "#MLP to transform points to 128 dimensions\n",
        "x = tf.keras.layers.Dense(128)(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "#Max aggregation function\n",
        "#At this point x has the shape [B, 512, 128], where B is the current batch size.\n",
        "#We want to end with a global feature tensor with shape [B,128] getting the \n",
        "#maximum value for each of the 128 features along the 512 points (axis 1). Use\n",
        "#the low-level API function tf.reduce_max for that.\n",
        "x = tf.reduce_max(x , axis=1)\n",
        "\n",
        "#Last MLP\n",
        "x = tf.keras.layers.Dense(32)(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.ReLU()(x)\n",
        " ################# END TODO \n",
        "\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ4LR8bhMmm6"
      },
      "source": [
        "Now we can train our model executing the following commands, and if everything went well, we will achieve an accuracy of around 90%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6J1lpJwJqWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a800ed1-e844-43d6-8c73-852fcbb8a162"
      },
      "source": [
        "#Create the model.\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='modelnet10_model')\n",
        "\n",
        "#Compile the model.\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.SGD(\n",
        "                  learning_rate=0.0001, \n",
        "                  momentum=0.98),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Fit the model to the data.\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=40,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "#Evaluate the model on the test data.\n",
        "model.evaluate(x_test, y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "624/624 [==============================] - 7s 7ms/step - loss: 1.5206 - accuracy: 0.5126 - val_loss: 1.0882 - val_accuracy: 0.6982\n",
            "Epoch 2/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.6627 - accuracy: 0.8162 - val_loss: 0.6743 - val_accuracy: 0.7797\n",
            "Epoch 3/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.4757 - accuracy: 0.8670 - val_loss: 0.5529 - val_accuracy: 0.7985\n",
            "Epoch 4/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.3872 - accuracy: 0.8911 - val_loss: 0.4910 - val_accuracy: 0.8238\n",
            "Epoch 5/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.3294 - accuracy: 0.9084 - val_loss: 0.4368 - val_accuracy: 0.8348\n",
            "Epoch 6/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.2868 - accuracy: 0.9205 - val_loss: 0.3810 - val_accuracy: 0.8656\n",
            "Epoch 7/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.2613 - accuracy: 0.9263 - val_loss: 0.3483 - val_accuracy: 0.8767\n",
            "Epoch 8/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.2330 - accuracy: 0.9336 - val_loss: 0.3300 - val_accuracy: 0.8833\n",
            "Epoch 9/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.2187 - accuracy: 0.9350 - val_loss: 0.3272 - val_accuracy: 0.8778\n",
            "Epoch 10/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1968 - accuracy: 0.9414 - val_loss: 0.3018 - val_accuracy: 0.8965\n",
            "Epoch 11/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1805 - accuracy: 0.9472 - val_loss: 0.3172 - val_accuracy: 0.8822\n",
            "Epoch 12/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1754 - accuracy: 0.9448 - val_loss: 0.3039 - val_accuracy: 0.8921\n",
            "Epoch 13/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1678 - accuracy: 0.9482 - val_loss: 0.2984 - val_accuracy: 0.8888\n",
            "Epoch 14/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1618 - accuracy: 0.9498 - val_loss: 0.3096 - val_accuracy: 0.8866\n",
            "Epoch 15/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1578 - accuracy: 0.9523 - val_loss: 0.2677 - val_accuracy: 0.9020\n",
            "Epoch 16/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1506 - accuracy: 0.9524 - val_loss: 0.2696 - val_accuracy: 0.9075\n",
            "Epoch 17/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1472 - accuracy: 0.9554 - val_loss: 0.2833 - val_accuracy: 0.8910\n",
            "Epoch 18/40\n",
            "624/624 [==============================] - 4s 7ms/step - loss: 0.1390 - accuracy: 0.9586 - val_loss: 0.2822 - val_accuracy: 0.8833\n",
            "Epoch 19/40\n",
            "624/624 [==============================] - 4s 7ms/step - loss: 0.1375 - accuracy: 0.9566 - val_loss: 0.2966 - val_accuracy: 0.8921\n",
            "Epoch 20/40\n",
            "624/624 [==============================] - 4s 7ms/step - loss: 0.1354 - accuracy: 0.9594 - val_loss: 0.2824 - val_accuracy: 0.9108\n",
            "Epoch 21/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1356 - accuracy: 0.9570 - val_loss: 0.3598 - val_accuracy: 0.8689\n",
            "Epoch 22/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1283 - accuracy: 0.9576 - val_loss: 0.2709 - val_accuracy: 0.8965\n",
            "Epoch 23/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1183 - accuracy: 0.9626 - val_loss: 0.2584 - val_accuracy: 0.9130\n",
            "Epoch 24/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1192 - accuracy: 0.9616 - val_loss: 0.2951 - val_accuracy: 0.8954\n",
            "Epoch 25/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1188 - accuracy: 0.9622 - val_loss: 0.2701 - val_accuracy: 0.8943\n",
            "Epoch 26/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1177 - accuracy: 0.9618 - val_loss: 0.2688 - val_accuracy: 0.9031\n",
            "Epoch 27/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1137 - accuracy: 0.9626 - val_loss: 0.3073 - val_accuracy: 0.8767\n",
            "Epoch 28/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1151 - accuracy: 0.9636 - val_loss: 0.2505 - val_accuracy: 0.9086\n",
            "Epoch 29/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1061 - accuracy: 0.9652 - val_loss: 0.3390 - val_accuracy: 0.8722\n",
            "Epoch 30/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1030 - accuracy: 0.9663 - val_loss: 0.3307 - val_accuracy: 0.8778\n",
            "Epoch 31/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1003 - accuracy: 0.9682 - val_loss: 0.2870 - val_accuracy: 0.8855\n",
            "Epoch 32/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1040 - accuracy: 0.9653 - val_loss: 0.2583 - val_accuracy: 0.9042\n",
            "Epoch 33/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1026 - accuracy: 0.9671 - val_loss: 0.2966 - val_accuracy: 0.8844\n",
            "Epoch 34/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.0959 - accuracy: 0.9684 - val_loss: 0.3103 - val_accuracy: 0.8833\n",
            "Epoch 35/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.1037 - accuracy: 0.9653 - val_loss: 0.3051 - val_accuracy: 0.8899\n",
            "Epoch 36/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.0948 - accuracy: 0.9679 - val_loss: 0.3799 - val_accuracy: 0.8700\n",
            "Epoch 37/40\n",
            "624/624 [==============================] - 4s 7ms/step - loss: 0.0970 - accuracy: 0.9663 - val_loss: 0.2528 - val_accuracy: 0.9185\n",
            "Epoch 38/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.0960 - accuracy: 0.9687 - val_loss: 0.2838 - val_accuracy: 0.8921\n",
            "Epoch 39/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.0970 - accuracy: 0.9679 - val_loss: 0.3106 - val_accuracy: 0.8899\n",
            "Epoch 40/40\n",
            "624/624 [==============================] - 4s 6ms/step - loss: 0.0879 - accuracy: 0.9707 - val_loss: 0.2552 - val_accuracy: 0.9097\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2551615238189697, 0.9096916317939758]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooeyu8jyFM-3"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# PointNet (Segmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmzfxssp4iu8"
      },
      "source": [
        "In this section, we will implement the version of PointNet architecture used to segment point clouds from the **ShapeNet Part** dataset (https://shapenet.cs.stanford.edu/iccv17/). This dataset is composed of 16K models from 16 different categories with 50 different object parts in total.\r\n",
        "\r\n",
        "The following command will download the preprocessed dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzkU_Qo5F9gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef9f18c-386a-49e5-9cbd-e35e48f7f24f"
      },
      "source": [
        "!wget https://www.dropbox.com/s/taos2s389ikli6d/shapenet.zip\r\n",
        "!unzip shapenet.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-20 09:35:52--  https://www.dropbox.com/s/taos2s389ikli6d/shapenet.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.67.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.67.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/taos2s389ikli6d/shapenet.zip [following]\n",
            "--2021-01-20 09:35:52--  https://www.dropbox.com/s/raw/taos2s389ikli6d/shapenet.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc557e3e614ad264ffa298311a56.dl.dropboxusercontent.com/cd/0/inline/BHX7t6VoQue-tv1KMThIhN9tsMUlYEXc4kDynL-BPKPn3CsfVM8NuvS2wLWTFfDzzMZrX9_ffT7gxMTywwpxF5g-WtpmSSM1O3vUuNaRfZedDtFzyVi5rU13q8xkP0JHXfM/file# [following]\n",
            "--2021-01-20 09:35:53--  https://uc557e3e614ad264ffa298311a56.dl.dropboxusercontent.com/cd/0/inline/BHX7t6VoQue-tv1KMThIhN9tsMUlYEXc4kDynL-BPKPn3CsfVM8NuvS2wLWTFfDzzMZrX9_ffT7gxMTywwpxF5g-WtpmSSM1O3vUuNaRfZedDtFzyVi5rU13q8xkP0JHXfM/file\n",
            "Resolving uc557e3e614ad264ffa298311a56.dl.dropboxusercontent.com (uc557e3e614ad264ffa298311a56.dl.dropboxusercontent.com)... 162.125.67.15, 2620:100:6020:15::a27d:400f\n",
            "Connecting to uc557e3e614ad264ffa298311a56.dl.dropboxusercontent.com (uc557e3e614ad264ffa298311a56.dl.dropboxusercontent.com)|162.125.67.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BHVrx6nJgHnUKrYygDJYM33oMqVXwHPZnrCye4WPMphAn2hZ2QeNhx1gb9vpP-hcb8CTnNdNamxYzYNvK2W_wxHdu2aOzMFRiE_txwhlF0VzyQeBjLs09_Xbrt9TAOFowY4ZhYEOyOgkbIL-DuujmKtWcrckPWn7iBAu17J0Y5hpAh-0HOgz1B-SEXwi1fYFEg8TMhf7MX29HYcFIrW74HgnVJic3pBS4chsvDI_uPugJbWBgsraKxCCJGM__F33oheVlC-1VIaH6X6wO-NcwShb2PPpZOTtFoK9GtSeWgPUSBxOlBznVAXPHRUu_KOp0RygUAlVha0LQH1UunbRlSQdH8dNfWWjt0PgsGMW3HqvLQ/file [following]\n",
            "--2021-01-20 09:35:53--  https://uc557e3e614ad264ffa298311a56.dl.dropboxusercontent.com/cd/0/inline2/BHVrx6nJgHnUKrYygDJYM33oMqVXwHPZnrCye4WPMphAn2hZ2QeNhx1gb9vpP-hcb8CTnNdNamxYzYNvK2W_wxHdu2aOzMFRiE_txwhlF0VzyQeBjLs09_Xbrt9TAOFowY4ZhYEOyOgkbIL-DuujmKtWcrckPWn7iBAu17J0Y5hpAh-0HOgz1B-SEXwi1fYFEg8TMhf7MX29HYcFIrW74HgnVJic3pBS4chsvDI_uPugJbWBgsraKxCCJGM__F33oheVlC-1VIaH6X6wO-NcwShb2PPpZOTtFoK9GtSeWgPUSBxOlBznVAXPHRUu_KOp0RygUAlVha0LQH1UunbRlSQdH8dNfWWjt0PgsGMW3HqvLQ/file\n",
            "Reusing existing connection to uc557e3e614ad264ffa298311a56.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 293310450 (280M) [application/zip]\n",
            "Saving to: ‘shapenet.zip’\n",
            "\n",
            "shapenet.zip        100%[===================>] 279.72M  23.1MB/s    in 14s     \n",
            "\n",
            "2021-01-20 09:36:08 (20.1 MB/s) - ‘shapenet.zip’ saved [293310450/293310450]\n",
            "\n",
            "Archive:  shapenet.zip\n",
            "  inflating: shapenet.hdf5           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoWRQWBfTL79"
      },
      "source": [
        "Once downloaded, we will prepare the data for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FStd8olqGX-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28cbfcc-d0e4-4b57-9000-ed6967c94e27"
      },
      "source": [
        "dataset_shapenet = h5py.File(\"shapenet.hdf5\", \"r\")\r\n",
        "\r\n",
        "#Training data.\r\n",
        "x_train = dataset_shapenet['train_data'][:] # 3D point coordinates.\r\n",
        "y_train = dataset_shapenet['train_labels'][:] # Point label (0-50).\r\n",
        "\r\n",
        "#Validation data.\r\n",
        "x_val = dataset_shapenet['val_data'][:] # 3D point coordinates.\r\n",
        "y_val = dataset_shapenet['val_labels'][:] # Point label (0-50).\r\n",
        "\r\n",
        "#Test data.\r\n",
        "x_test = dataset_shapenet['test_data'][:] # 3D point coordinates.\r\n",
        "y_test = dataset_shapenet['test_labels'][:] # Point label (0-50).\r\n",
        "\r\n",
        "print(\"Point cloud training:\", x_train.shape[0])\r\n",
        "print(\"Point cloud validation:\", x_val.shape[0])\r\n",
        "print(\"Point cloud testing:\", x_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Point cloud training: 12137\n",
            "Point cloud validation: 1870\n",
            "Point cloud testing: 2874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItDfGicVTVkX"
      },
      "source": [
        "Now we are ready to create our network architecture. We are going to design a similar architecture as in the previous section but here we will use it for point cloud segmentation. Therefore, we will need to do some changes.\r\n",
        "\r\n",
        "Since we are doing predictions per point, we will need to concatenate the global descriptor obtained with the max-pooling to the individual features of each point. We will select as individual features the 128 features of the second layer after applying the activation function.\r\n",
        "\r\n",
        "Once we concatenate the local and global information, we will process these features by another hidden layer of 128 features before predicting the final probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiMxRh_4G9dy"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(512, 3), name='batch_point_cloud')\r\n",
        "\r\n",
        "################# TODO \r\n",
        "#MLP to transform points to 64 dimensions\r\n",
        "x = tf.keras.layers.Dense(64)(inputs)\r\n",
        "x = tf.keras.layers.BatchNormalization()(x)\r\n",
        "x = tf.keras.layers.ReLU()(x)\r\n",
        "x = tf.keras.layers.Dropout(x)\r\n",
        "\r\n",
        "\r\n",
        "#MLP to transform points to 128 dimensions\r\n",
        "x = tf.keras.layers.Dense(128)(inputs)\r\n",
        "x = tf.keras.layers.BatchNormalization()(x)\r\n",
        "x = tf.keras.layers.ReLU()(x)\r\n",
        "x = tf.keras.layers.Dropout(x)\r\n",
        "y = x\r\n",
        "\r\n",
        "#Max aggregation function\r\n",
        "#At this point x has the shape [B, 512, 128], where B is the current batch size.\r\n",
        "#We want to end with a global feature tensor with shape [B,128] getting the \r\n",
        "#maximum value for each of the 128 features along the 512 points (axis 1). Use\r\n",
        "#the low-level API function tf.reduce_max for that.\r\n",
        "x = tf.reduce_max(x , axis=1)\r\n",
        "\r\n",
        "#Concatenate\r\n",
        "#Here we have to concatenate x and y. For that we first will need to reshape y \r\n",
        "#to [B, 1, 128] and then use tf.tile to replicate axis one 512 times.\r\n",
        "z = tf.keras.layers.Reshape((1,128))(x)\r\n",
        "x = tf.tile(z , [1,512,1])\r\n",
        "x = tf.concat([x,y] , 2)\r\n",
        "\r\n",
        "#Last MLP\r\n",
        "x = tf.keras.layers.Dense(128)(x)\r\n",
        "x = tf.keras.layers.BatchNormalization()(x)\r\n",
        "x = tf.keras.layers.ReLU()(x)\r\n",
        "x = tf.keras.layers.Dropout(x)\r\n",
        "################# END TODO \r\n",
        "\r\n",
        "outputs = tf.keras.layers.Dense(50, activation='softmax')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQOY1VJVUNW6"
      },
      "source": [
        "Lastly, we will train the model with the **ShapeNet Part** dataset. You should obtain an accuracy of around 84% in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8f94XdTI0jt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f27bc13-eb77-47bf-cca8-9db260cce20d"
      },
      "source": [
        "#Create the model.\r\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='shapenet_model')\r\n",
        "\r\n",
        "#Compile the model.\r\n",
        "model.compile(loss='sparse_categorical_crossentropy',\r\n",
        "              optimizer=tf.keras.optimizers.SGD(\r\n",
        "                  learning_rate=0.0001, \r\n",
        "                  momentum=0.98),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "#Fit the model to the data.\r\n",
        "model.fit(x_train, y_train,\r\n",
        "          batch_size=32,\r\n",
        "          epochs=40,\r\n",
        "          validation_data=(x_val, y_val))\r\n",
        "\r\n",
        "#Evaluate the model on the test data.\r\n",
        "model.evaluate(x_test, y_test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "380/380 [==============================] - 4s 9ms/step - loss: 3.1906 - accuracy: 0.2681 - val_loss: 3.3373 - val_accuracy: 0.1861\n",
            "Epoch 2/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.3969 - accuracy: 0.6552 - val_loss: 1.1594 - val_accuracy: 0.7284\n",
            "Epoch 3/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.1146 - accuracy: 0.7252 - val_loss: 0.9207 - val_accuracy: 0.7629\n",
            "Epoch 4/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.0006 - accuracy: 0.7460 - val_loss: 0.8340 - val_accuracy: 0.7793\n",
            "Epoch 5/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.9200 - accuracy: 0.7613 - val_loss: 0.7721 - val_accuracy: 0.7928\n",
            "Epoch 6/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.8684 - accuracy: 0.7688 - val_loss: 0.7300 - val_accuracy: 0.8005\n",
            "Epoch 7/40\n",
            "380/380 [==============================] - 3s 9ms/step - loss: 0.8404 - accuracy: 0.7751 - val_loss: 0.6939 - val_accuracy: 0.8078\n",
            "Epoch 8/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.7849 - accuracy: 0.7849 - val_loss: 0.6639 - val_accuracy: 0.8141\n",
            "Epoch 9/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.7522 - accuracy: 0.7909 - val_loss: 0.6396 - val_accuracy: 0.8170\n",
            "Epoch 10/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.7175 - accuracy: 0.7996 - val_loss: 0.6191 - val_accuracy: 0.8200\n",
            "Epoch 11/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.6966 - accuracy: 0.8047 - val_loss: 0.6030 - val_accuracy: 0.8244\n",
            "Epoch 12/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.6949 - accuracy: 0.8023 - val_loss: 0.5902 - val_accuracy: 0.8235\n",
            "Epoch 13/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.6643 - accuracy: 0.8081 - val_loss: 0.5765 - val_accuracy: 0.8270\n",
            "Epoch 14/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.6466 - accuracy: 0.8110 - val_loss: 0.5660 - val_accuracy: 0.8295\n",
            "Epoch 15/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.6263 - accuracy: 0.8156 - val_loss: 0.5527 - val_accuracy: 0.8300\n",
            "Epoch 16/40\n",
            "380/380 [==============================] - 3s 9ms/step - loss: 0.6251 - accuracy: 0.8150 - val_loss: 0.5478 - val_accuracy: 0.8325\n",
            "Epoch 17/40\n",
            "380/380 [==============================] - 4s 9ms/step - loss: 0.6259 - accuracy: 0.8129 - val_loss: 0.5339 - val_accuracy: 0.8343\n",
            "Epoch 18/40\n",
            "380/380 [==============================] - 4s 9ms/step - loss: 0.6118 - accuracy: 0.8170 - val_loss: 0.5285 - val_accuracy: 0.8333\n",
            "Epoch 19/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5982 - accuracy: 0.8206 - val_loss: 0.5194 - val_accuracy: 0.8376\n",
            "Epoch 20/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.6034 - accuracy: 0.8160 - val_loss: 0.5135 - val_accuracy: 0.8389\n",
            "Epoch 21/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5874 - accuracy: 0.8221 - val_loss: 0.5083 - val_accuracy: 0.8424\n",
            "Epoch 22/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5811 - accuracy: 0.8228 - val_loss: 0.4980 - val_accuracy: 0.8429\n",
            "Epoch 23/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5660 - accuracy: 0.8266 - val_loss: 0.4970 - val_accuracy: 0.8421\n",
            "Epoch 24/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5649 - accuracy: 0.8265 - val_loss: 0.4914 - val_accuracy: 0.8446\n",
            "Epoch 25/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5663 - accuracy: 0.8265 - val_loss: 0.4909 - val_accuracy: 0.8424\n",
            "Epoch 26/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5603 - accuracy: 0.8248 - val_loss: 0.4801 - val_accuracy: 0.8472\n",
            "Epoch 27/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5428 - accuracy: 0.8312 - val_loss: 0.4869 - val_accuracy: 0.8444\n",
            "Epoch 28/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5342 - accuracy: 0.8334 - val_loss: 0.4751 - val_accuracy: 0.8496\n",
            "Epoch 29/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5322 - accuracy: 0.8336 - val_loss: 0.4724 - val_accuracy: 0.8511\n",
            "Epoch 30/40\n",
            "380/380 [==============================] - 3s 9ms/step - loss: 0.5314 - accuracy: 0.8335 - val_loss: 0.4734 - val_accuracy: 0.8465\n",
            "Epoch 31/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5327 - accuracy: 0.8347 - val_loss: 0.4670 - val_accuracy: 0.8491\n",
            "Epoch 32/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5256 - accuracy: 0.8353 - val_loss: 0.4594 - val_accuracy: 0.8527\n",
            "Epoch 33/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5167 - accuracy: 0.8370 - val_loss: 0.4583 - val_accuracy: 0.8498\n",
            "Epoch 34/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5339 - accuracy: 0.8337 - val_loss: 0.4566 - val_accuracy: 0.8549\n",
            "Epoch 35/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5019 - accuracy: 0.8403 - val_loss: 0.4506 - val_accuracy: 0.8549\n",
            "Epoch 36/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5138 - accuracy: 0.8369 - val_loss: 0.4483 - val_accuracy: 0.8537\n",
            "Epoch 37/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5010 - accuracy: 0.8421 - val_loss: 0.4498 - val_accuracy: 0.8560\n",
            "Epoch 38/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5141 - accuracy: 0.8375 - val_loss: 0.4461 - val_accuracy: 0.8522\n",
            "Epoch 39/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5019 - accuracy: 0.8418 - val_loss: 0.4425 - val_accuracy: 0.8540\n",
            "Epoch 40/40\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 0.5048 - accuracy: 0.8392 - val_loss: 0.4408 - val_accuracy: 0.8569\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5521785616874695, 0.8241738677024841]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}